{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing custom model analysis results\n",
    "\n",
    "This notebook demonstrates how to use dictionaries for noramalizing NLU results from a custom language model that was ceated from those same dictionaries.\n",
    "\n",
    "- Step 1: Import saved analysis results\n",
    "- Step 2: Load dictionaries\n",
    "- Step 3: Normalize results\n",
    "- Step 5: Save normalized results in Watson Studio project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import saved analysis results\n",
    "\n",
    "In the [Custom language model](https://github.com/spackows/CASCON-2019_NLP-workshops/blob/master/notebooks/Notebook-3_Custom-language-model.ipynb) notebook, we saved custom language results in a JSON file as a Project Asset.\n",
    "\n",
    "To import the saved data into this notebook, perform these steps:\n",
    "1. Open the data panel by clicking on the **Find and Add Data** icon ( <img style=\"margin: 0px; padding: 0px; display: inline;\" src=\"https://github.com/spackows/CASCON-2019_NLP-workshops/raw/master/images/find-add-data-icon.png\"/> )\n",
    "2. Click on the empty cell below\n",
    "5. In the data panel, under the file named <code>NLU-results-custom-model.json</code> click **Insert to code** and then select \"Insert Credentials\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function for copying files from  \n",
    "# Project storage to the notebook working directory\n",
    "#\n",
    "\n",
    "from ibm_botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def copyToNotebookDir( credentials ):\n",
    "    cos = ibm_boto3.client(\n",
    "        service_name='s3',\n",
    "        ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n",
    "        ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n",
    "        ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n",
    "        config=Config(signature_version='oauth'),\n",
    "        endpoint_url=credentials['ENDPOINT'])\n",
    "    cos.download_file(Bucket=credentials['BUCKET'],Key=credentials['FILE'],Filename=credentials['FILE'])\n",
    "    print( \"Done: '\" + credentials['FILE'] + \"'\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: 'NLU-results-custom-model.json'\n"
     ]
    }
   ],
   "source": [
    "copyToNotebookDir( credentials_1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'header': '-------------------------------------------------------------',\n",
       "  'message': 'Good morning can you help me upload a shapefile?',\n",
       "  'actions': ['upload'],\n",
       "  'objects': ['shapefile'],\n",
       "  'tech': [],\n",
       "  'docs': [],\n",
       "  'persona': [],\n",
       "  'spacer': ''},\n",
       " {'header': '-------------------------------------------------------------',\n",
       "  'message': 'Good night where to place my file to import it into notebook?',\n",
       "  'actions': ['import'],\n",
       "  'objects': ['notebook'],\n",
       "  'tech': [],\n",
       "  'docs': [],\n",
       "  'persona': [],\n",
       "  'spacer': ''},\n",
       " {'header': '-------------------------------------------------------------',\n",
       "  'message': 'hai how can i do analyze with csv file is there any tutorial on it',\n",
       "  'actions': ['analyze'],\n",
       "  'objects': [],\n",
       "  'tech': [],\n",
       "  'docs': ['tutorial'],\n",
       "  'persona': [],\n",
       "  'spacer': ''}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open( credentials_1['FILE'] ) as json_file:\n",
    "    raw_results_list = json.load(json_file)\n",
    "raw_results_list[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: View results\n",
    "\n",
    "View the action words, object words, and technology words, listed in order of occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function for counting words\n",
    "#\n",
    "from collections import OrderedDict\n",
    "def countWords( results_list, entity_type ):\n",
    "    all_words = {}\n",
    "    for result in results_list:\n",
    "        words_arr = result[entity_type]\n",
    "        for word in words_arr:\n",
    "            if( word not in all_words ):\n",
    "                all_words[word] = 0\n",
    "            all_words[word] += 1\n",
    "    common_words = dict( [ (k,v) for k,v in all_words.items() if v > 1 ] )\n",
    "    ordered_common_words = OrderedDict( sorted( common_words.items(), key=lambda x:x[1], reverse=True ) )\n",
    "    return ordered_common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_counts  = countWords( raw_results_list, \"actions\" )\n",
    "objects_counts = countWords( raw_results_list, \"objects\" )\n",
    "tech_counts    = countWords( raw_results_list, \"tech\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('create', 10),\n",
       "             ('upload', 5),\n",
       "             ('import', 3),\n",
       "             ('download', 3),\n",
       "             ('creating', 3),\n",
       "             ('add', 3),\n",
       "             ('connection', 3),\n",
       "             ('connect', 2),\n",
       "             ('training', 2),\n",
       "             ('export', 2),\n",
       "             ('deploy', 2),\n",
       "             ('signup', 2)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( \"\\nActions\" )\n",
    "actions_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Objects\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('notebook', 14),\n",
       "             ('project', 8),\n",
       "             ('model', 7),\n",
       "             ('account', 4),\n",
       "             ('Notebook', 3),\n",
       "             ('trial', 2)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( \"\\nObjects\" )\n",
    "objects_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Technologies\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('R', 4),\n",
       "             ('WML', 2),\n",
       "             ('Github', 2),\n",
       "             ('python', 2),\n",
       "             ('jupyter', 2),\n",
       "             ('spark', 2)])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( \"\\nTechnologies\" )\n",
    "tech_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Normalize results\n",
    "\n",
    "There are some noisy results above.  For example, in `actions_counts`, \"create\" and \"creating\" are counted as separate entities.  But for our analysis purposes, those both refer to the same action.  Instead of being counted separately, they should be counted together.\n",
    "\n",
    "#### Dictionary files\n",
    "\n",
    "To train the custom language model, we created dictionary files that looked like this:\n",
    "\n",
    "[Action words](https://raw.githubusercontent.com/spackows/CASCON-2019_NLP-workshops/master/custom-language-model/dictionaries/action.csv)\n",
    "\n",
    "```\n",
    "lemma,poscode,surface\n",
    "select,2,select,selecting\n",
    "create,2,create,creating\n",
    "train,2,train,training\n",
    "load,2,load,loading,upload,uploading\n",
    "sign up,2,sign up,sign-up,signup,register,registering\n",
    "import,2,import,importing,imported\n",
    "...\n",
    "```\n",
    "\n",
    "Given that we went to the trouble of creating those dictionaries, let's use them to *normalize* results.  For example, count \"create\" and \"creating\" as two instances of the same action.\n",
    "\n",
    "#### Method: _lookup_\n",
    "\n",
    "The way we'll use those dictionary files to normalize results is this:\n",
    "\n",
    "From the dictionaries, create an important words look-up structure that we can use to map any `surface` form back to the `lemma` form.\n",
    "\n",
    "For example, using the action words dictionary, \"loading\", \"upload\", and \"uploading\" should all map back to: \"load\".\n",
    "\n",
    "#### Other methods\n",
    "\n",
    "We could use *stemming* or *lemmatization* libraries.. But why?  We already have this dictionaries of words we care about, so let's just use those!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "def readSource( url ):\n",
    "    content = urllib.request.urlopen( url )\n",
    "    lines_arr = []\n",
    "    for line in content:\n",
    "        lines_arr.append( line.decode(\"utf-8\") )\n",
    "    return lines_arr\n",
    "\n",
    "def addLookups( lines_arr, lookup_dict ):\n",
    "    for i in range( 1, len( lines_arr ) ):\n",
    "        line = lines_arr[i]\n",
    "        line = re.sub( \"\\s+$\", \"\", line )\n",
    "        arr = line.split( \",\" )\n",
    "        lemma = arr[0].lower()\n",
    "        for j in range( 3, len( arr ) ):\n",
    "            variant = arr[j].lower()\n",
    "            if variant not in lookup_dict:\n",
    "                lookup_dict[ variant ] = lemma\n",
    "    return lookup_dict\n",
    "\n",
    "def readCustomDictionaries( url_arr ):\n",
    "    lookup_dict = {}\n",
    "    for url in url_arr:\n",
    "        lines_arr = readSource( url )\n",
    "        lookup_dict = addLookups( lines_arr, lookup_dict )\n",
    "    return lookup_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dict_url = \"https://raw.githubusercontent.com/spackows/CASCON-2019_NLP-workshops/master/custom-language-model/dictionaries/action.csv\"\n",
    "obj_dict_url    = \"https://raw.githubusercontent.com/spackows/CASCON-2019_NLP-workshops/master/custom-language-model/dictionaries/obj.csv\"\n",
    "tech_dict_url   = \"https://raw.githubusercontent.com/spackows/CASCON-2019_NLP-workshops/master/custom-language-model/dictionaries/tech.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'selecting': 'select',\n",
       " 'creating': 'create',\n",
       " 'training': 'train',\n",
       " 'loading': 'load',\n",
       " 'upload': 'load',\n",
       " 'uploading': 'load',\n",
       " 'sign-up': 'sign up',\n",
       " 'signup': 'sign up',\n",
       " 'register': 'sign up',\n",
       " 'registering': 'sign up',\n",
       " 'importing': 'import',\n",
       " 'imported': 'import',\n",
       " 'adding': 'add',\n",
       " 'recovering': 'recover',\n",
       " 'changing': 'change',\n",
       " 'building': 'build',\n",
       " 'login': 'log in',\n",
       " 'logging in': 'log in',\n",
       " 'sign in': 'log in',\n",
       " 'signing in': 'log in',\n",
       " 'sign-in': 'log in',\n",
       " 'signin': 'log in',\n",
       " 'connecting': 'connect',\n",
       " 'connection': 'connect',\n",
       " 'connections': 'connect',\n",
       " 'deploys': 'deploy',\n",
       " 'deployed': 'deploy',\n",
       " 'deploying': 'deploy',\n",
       " 'setting up': 'set up',\n",
       " 'setup': 'set up',\n",
       " 'set-up': 'set up',\n",
       " 'editing': 'edit',\n",
       " 'exceeds': 'exceed',\n",
       " 'exceeded': 'exceed',\n",
       " 'exceeding': 'exceed',\n",
       " 'exporting': 'export',\n",
       " 'analyzing': 'analyze',\n",
       " 'downloading': 'download',\n",
       " 'accessing': 'access',\n",
       " 'acess': 'access',\n",
       " 'saving': 'save',\n",
       " 'initiating': 'initiate',\n",
       " 'preparing': 'prepare',\n",
       " 'requesting': 'request',\n",
       " 'writing': 'write',\n",
       " 'renaming': 'rename',\n",
       " 'orgs': 'org',\n",
       " 'organization': 'org',\n",
       " 'organizations': 'org',\n",
       " 'models': 'model',\n",
       " 'accounts': 'account',\n",
       " 'acount': 'account',\n",
       " 'acont': 'account',\n",
       " 'accont': 'account',\n",
       " 'accout': 'account',\n",
       " 'projects': 'project',\n",
       " 'names': 'name',\n",
       " 'datasets': 'dataset',\n",
       " 'data set': 'dataset',\n",
       " 'data sets': 'dataset',\n",
       " 'shapefiles': 'shapefile',\n",
       " 'shape file': 'shapefile',\n",
       " 'shape files': 'shapefile',\n",
       " 'access keys': 'access key',\n",
       " 'endpoints': 'endpoint',\n",
       " 'end point': 'endpoint',\n",
       " 'end points': 'endpoint',\n",
       " 'scoring endpoint': 'endpoint',\n",
       " 'scoring endpoints': 'endpoint',\n",
       " 'scoring end point': 'endpoint',\n",
       " 'scoring end points': 'endpoint',\n",
       " 'limits': 'limit',\n",
       " 'free trial': 'trial',\n",
       " 'trail': 'trial',\n",
       " 'free plan': 'trial',\n",
       " 'free access': 'trial',\n",
       " 'collaborators': 'collaborator',\n",
       " 'flows': 'flow',\n",
       " 'data frame': 'dataframe',\n",
       " 'databases': 'database',\n",
       " 'db': 'database',\n",
       " 'dbm': 'database',\n",
       " 'dbs': 'database',\n",
       " 'note book': 'notebook',\n",
       " 'note-book': 'notebook',\n",
       " 'notebooks': 'notebook',\n",
       " 'note-books': 'notebook',\n",
       " 'note books': 'notebook',\n",
       " 'watson machine learning': 'machine learning',\n",
       " 'ibm watson machine learning': 'machine learning',\n",
       " 'ibm watson machine learning (spss)': 'machine learning',\n",
       " 'spss': 'machine learning',\n",
       " 'spss modeler': 'machine learning',\n",
       " 'modeler': 'machine learning',\n",
       " 'wml': 'machine learning',\n",
       " 'ml': 'machine learning',\n",
       " 'watsonml': 'machine learning',\n",
       " 'r studio': 'r',\n",
       " 'rstudio': 'r',\n",
       " 'r-studio': 'r',\n",
       " 'object-storage': 'object storage',\n",
       " 'objectstorage': 'object storage',\n",
       " 'apis': 'api',\n",
       " 'api': 'api',\n",
       " 'sparks': 'spark',\n",
       " 'pyspark': 'spark',\n",
       " 'git': 'github',\n",
       " 'clodant': 'cloudant',\n",
       " 'cloudent nosql': 'cloudant',\n",
       " 'cloudent nosql db': 'cloudant',\n",
       " 'nodered': 'node-red',\n",
       " 'node red': 'node-red',\n",
       " 'db2': 'db2',\n",
       " 'dash': 'db2',\n",
       " 'dashdb': 'db2'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_struct = readCustomDictionaries( [ action_dict_url, obj_dict_url, tech_dict_url ] )\n",
    "lookup_struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize results\n",
    "#\n",
    "\n",
    "def normalize( word, lookup_struct ):\n",
    "    if word in lookup_struct:\n",
    "        return lookup_struct[word]\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "normalized_results_list = []\n",
    "for result in raw_results_list:\n",
    "    actions_arr = []\n",
    "    for action in result[\"actions\"]:\n",
    "        actions_arr.append( normalize( action.lower(), lookup_struct ) )\n",
    "    objects_arr = []\n",
    "    for obj in result[\"objects\"]:\n",
    "        objects_arr.append( normalize( obj.lower(), lookup_struct ) )\n",
    "    tech_arr = []\n",
    "    for tech in result[\"tech\"]:\n",
    "        tech_arr.append( normalize( tech.lower(), lookup_struct ) )\n",
    "    normalized_results_list.append( { \"header\"  : result[\"header\"],\n",
    "                                      \"message\" : result[\"message\"],\n",
    "                                      \"actions\" : actions_arr,\n",
    "                                      \"objects\" : objects_arr,\n",
    "                                      \"tech\"    : tech_arr,\n",
    "                                      \"docs\"    : result[\"docs\"],\n",
    "                                      \"persona\" : result[\"persona\"],\n",
    "                                      \"spacer\"  : result[\"spacer\"] } )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_counts_normalized  = countWords( normalized_results_list, \"actions\" )\n",
    "objects_counts_normalized = countWords( normalized_results_list, \"objects\" )\n",
    "tech_counts_normalized    = countWords( normalized_results_list, \"tech\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actions (normalized)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('create', 14),\n",
       "             ('load', 7),\n",
       "             ('connect', 6),\n",
       "             ('import', 4),\n",
       "             ('download', 3),\n",
       "             ('add', 3),\n",
       "             ('sign up', 3),\n",
       "             ('log in', 2),\n",
       "             ('exceed', 2),\n",
       "             ('access', 2),\n",
       "             ('train', 2),\n",
       "             ('export', 2),\n",
       "             ('deploy', 2)])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( \"\\nActions (normalized)\" )\n",
    "actions_counts_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Objects (normalized)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('notebook', 19),\n",
       "             ('model', 8),\n",
       "             ('project', 8),\n",
       "             ('account', 5),\n",
       "             ('trial', 5),\n",
       "             ('limit', 2),\n",
       "             ('endpoint', 2)])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( \"\\nObjects (normalized)\" )\n",
    "objects_counts_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Technologies (normalized)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('machine learning', 8),\n",
       "             ('r', 6),\n",
       "             ('spark', 4),\n",
       "             ('cloudant', 3),\n",
       "             ('object storage', 3),\n",
       "             ('github', 2),\n",
       "             ('api', 2),\n",
       "             ('python', 2),\n",
       "             ('jupyter', 2)])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( \"\\nTechnologies (normalized)\" )\n",
    "tech_counts_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Save results\n",
    "\n",
    "Save NLU custom model results in a JSON file as a Project Asset.\n",
    "\n",
    "To be able to easily save questions in .csv files as assets in our Watson Studio project, we need a project token.\n",
    "\n",
    "Follow the steps in this topic: [Adding a project token](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/token.html?audience=wdp&context=data)\n",
    "\n",
    "***The project token is added in the very first cell at the top of the notebook.  Don't forget to scroll up and run that cell.***\n",
    "\n",
    "(If you forget to run the inserted cell, you'll see the error <code>name 'project' is not defined</code> when you try to run the next cell below.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'NLU-results-custom-model-normalized.json',\n",
       " 'message': 'File saved to project storage.',\n",
       " 'bucket_name': 'cascon2019-donotdelete-pr-gsnhbqe4skdcxh',\n",
       " 'asset_id': '4a5a476b-ce32-49a6-91de-ae09d12c4dfd'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.save_data( 'NLU-results-custom-model-normalized.json', json.dumps( normalized_results_list, indent=3 ) , overwrite=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright Â© 2019 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
